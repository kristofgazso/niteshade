<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>About &mdash; niteshade 0+unknown documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="#" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" />
    <link rel="prev" title="niteshade Docs" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">About</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adversarial-machine-learning">Adversarial Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#online-machine-learning">Online Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-poisoning-attacks-against-online-learning">Data Poisoning Attacks Against Online Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#defending-against-data-poisoning-attacks">Defending Against Data Poisoning Attacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-niteshade">What is niteshade?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributors.html">Contributors</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">niteshade</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>About</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/about.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="about">
<h1>About<a class="headerlink" href="#about" title="Permalink to this headline"></a></h1>
<section id="adversarial-machine-learning">
<span id="id1"></span><h2>Adversarial Machine Learning<a class="headerlink" href="#adversarial-machine-learning" title="Permalink to this headline"></a></h2>
<p>As machine learning systems become increasingly ubiquitous, so do malicious
actors seeking to exploit them. <strong>Adversarial machine learning</strong> refers to
techniques attempting to exploit model vulnerabilities by leveraging available
model information through hostile attacks. Many attack scenarios exist, but
some of the most commonly encountered ones are:</p>
<ul class="simple">
<li><p>Evasion attacks,</p></li>
<li><p>Model extraction attacks, and</p></li>
<li><p>Data poisoning attacks</p></li>
</ul>
<p>With machine learning systems rapidly becoming a core component of many
organisations’ day-to-day operations, the need to protect them against such
attacks is growing.</p>
</section>
<section id="online-machine-learning">
<span id="id2"></span><h2>Online Machine Learning<a class="headerlink" href="#online-machine-learning" title="Permalink to this headline"></a></h2>
<p>In contrast to offline machine learning, in which models are trained using a
static training dataset in its entirety all at once, <strong>online machine
learning</strong> is a method in which data becomes available dynamically in a
sequential order, and models are trained incrementally. In some scenarios,
the ordering of the data may be of importance (e.g. time series forecasting,
sequence-to-sequence modelling) and in others it may not be (e.g. minibatch
gradient descent for a neural network classifier), but an online data pipeline
may be employed because training over the entire training dataset at once is
computationally infeasible. Regardless of whether the data sequence is of
importance, online learning poses unique vulnerabilities as compared with
offline learning and merits special consideration from a security standpoint.</p>
</section>
<section id="data-poisoning-attacks-against-online-learning">
<span id="data-poisoning-attacks"></span><h2>Data Poisoning Attacks Against Online Learning<a class="headerlink" href="#data-poisoning-attacks-against-online-learning" title="Permalink to this headline"></a></h2>
<p><strong>Data poisoning attacks</strong> are a specific class of adversarial machine learning
attack in which an adversary alters a portion of a victim model’s training data
(generally by adding, perturbing or removing points) in order to satisfy some
nefarious objective (e.g. flipping labels so that a classifier systematically
misclassifies a particular object). Data poisoning attacks against online
learning are of particular concern, as many online learning systems make
decisions in real-time and therefore corrupted input data can lead to drastic
immediate consequences (e.g. perturbing stock price data may cause a trading
bot to make unprofitable trades, painting over stop signs may cause an
autonomous vehicle to drive recklessly).</p>
</section>
<section id="defending-against-data-poisoning-attacks">
<span id="id3"></span><h2>Defending Against Data Poisoning Attacks<a class="headerlink" href="#defending-against-data-poisoning-attacks" title="Permalink to this headline"></a></h2>
<p>Just as there are many different strategies for deploying data poisoning
attacks against online learning systems, there numerous strategies for
defending against them. Defending against data poisoning attacks generally
involves attempting to minimise damage by identifying suspicious datapoints and
either removing them from the training data pipeline or adjusting values so
that they fall in a more “reasonable” range. Additionally, regularisation may
be used in various forms to make models less sensitive to the data on which
they are trained. The effectiveness of a defence strategy depends on the attack
strategy against which it is defending and how well the defence parameters are
calibrated.</p>
</section>
<section id="what-is-niteshade">
<span id="id4"></span><h2>What is niteshade?<a class="headerlink" href="#what-is-niteshade" title="Permalink to this headline"></a></h2>
<p>niteshade is an open-source Python library for simulating data poisoning
attacks and defences against online machine learning systems. The library
provides a framework for cybersecurity researchers, professionals and
enthusiasts to simulate adversarial learning scenarios using a simple and
intuitive API. niteshade offers several out-of-the-box attack and defence
strategy classes, as well as a well-defined class hierarchy which makes it easy
for users to define their own attack and defence strategies. Workflows are
heavily integrated with PyTorch; data pipelines are specifically designed for
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> data types and models inherit from <code class="docutils literal notranslate"><span class="pre">torch.nn.module</span></code>.
niteshade’s postprocessing module allows users to easily assess the
effectiveness of attack and defence strategies by computing metrics and KPI’s,
plotting results and generating summary reports.</p>
</section>
<section id="references">
<span id="id5"></span><h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p>Some good references for those wishing to dig deeper into the mathematical
details of data poisoning attacks against online learning as well as defence
strategies:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-98842-9_3">https://link.springer.com/chapter/10.1007/978-3-319-98842-9_3</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1808.08994">https://arxiv.org/abs/1808.08994</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1903.01666">https://arxiv.org/abs/1903.01666</a></p></li>
<li><p><a class="reference external" href="https://www.doc.ic.ac.uk/~lmunozgo/publication/6poisoning-online-esann/">https://www.doc.ic.ac.uk/~lmunozgo/publication/6poisoning-online-esann/</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1905.12121">https://arxiv.org/abs/1905.12121</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="niteshade Docs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Mart Bakler, Oskar Fernlund, Alex Ntemourtsidou, Jaime Sabal, Mustafa Saleem.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>